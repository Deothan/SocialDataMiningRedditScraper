from bs4 import BeautifulSoup
from urllib.request import urlopen
import re
import time
import pandas as pd


#Define constants
#Base URL for subreddit you want to scrape
BASE_URL = "https://www.reddit.com/r/technology/controversial/?t=all"
TEST_URL = "https://www.reddit.com/r/technology/controversial/?t=all&count=998&after=t3_6oq8g"
count = 0
posts = []
lastpost =""
waittime = 0;

#Create a data frame from data and writes to csv
def pandaDataFrama():
    df = pd.DataFrame(posts,columns=["title","votes","author","url","timestamp","domain","text"])
    print("making csv")
    df.to_csv("pandaData.csv")


#Makes a BeautifulSoup for the url requested. has a waiting time for 10 sec if an error occurs
# most likely a lot of HTTP 429 errors will occur because of a lot of requests
def makesoup(section_url):
    soup = ""
    while(True):
        try:
            html = urlopen(section_url).read()
            soup = BeautifulSoup(html, "lxml")
            print("made soup for:",section_url)
            break
        except:
            print("sleeping for 1 seconds","waiting for: "+section_url)
            waittime+1
            time.sleep(1)

    return soup

#Gets the text for a reddit post
def getPostText(post_url):
    soup = makesoup(post_url)
    for div in soup.find_all("div","expando expando-uninitialized"):
        for text in div.find_all("div","md"):
            return text.get_text()


#Gets post data from main subreddit.
def getPostLinks(section_url,amount,lastpost="",count=0):
    soup = makesoup(section_url)

    #Because all reddit posts and list or autogenerated some regex has to be used
    #Find the list of links from reddit defined form a div with class starting with "thing"
    for s in soup.find_all("div",re.compile("^sitetable")):
        try:
            test = s.p["id"]

            if test =="noresults":
                print("no results")
                return
        except:
            continue
    sitetable = soup.find_all("div",re.compile("^thing"))

    if count<amount:
        for s in sitetable:

            #no reason to get a standard message
            if s["data-author"] !="AutoModerator":
                reddit = []

                #finds the title of the post
                unvoted_entry = s.find_all("div","entry unvoted")
                for child in unvoted_entry:
                    for title in child.find_all("a", re.compile("^title may-blank")):
                        reddit.append(title.string)

                # Finding the amount of votes for the post
                for midcol in s.find_all("div","midcol unvoted"):
                    for vote in midcol.find_all("div","score unvoted"):
                        if vote.string != "â€¢":
                            reddit.append(vote.string)
                        else:
                            reddit.append("unvoted")
                reddit.append(s["data-author"])

                #Creates a proper url for reddit posts
                if str(s["data-url"]).startswith("/r"):
                    print("found reddit post")
                    reddit.append("https://reddit.com"+s["data-url"])
                    local = True
                else:
                    local = False
                    reddit.append(s["data-url"])

                reddit.append(s["data-timestamp"])
                reddit.append(s["data-domain"])

                #if its a reddit post, get the text from it
                if local:
                    reddit.append(getPostText("https://wwww.reddit.com"+s["data-url"]))
                else:
                    reddit.append("Outside link")
                lastpost = s["data-fullname"]
                posts.append(reddit)

                #print(count)
            else: continue
            count += 1


        #Recusively call function till for next page
        newurl = BASE_URL+"&count="+str(count)+"&after="+str(lastpost)
        getPostLinks(newurl,amount,lastpost,count)


def scrapeReddit(amount):
    getPostLinks(BASE_URL, amount)
    pandaDataFrama()

    
#call this function with amount of posts you want.
scrapeReddit(1000)

