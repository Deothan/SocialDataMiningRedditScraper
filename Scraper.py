from bs4 import BeautifulSoup
from urllib.request import urlopen
import re
import time
import pandas as pd


#Define the constants
#Base URL for subreddit you want to scrape
BASE_URL = "https://www.reddit.com/r/technology/controversial/"
#The number of threads
NumberOfThreads = 50

#Don't touch.
count = 0
posts = []
lastpost =""

#Create a data frame from data and writes to csv
def pandaDataFrama():
    df = pd.DataFrame(posts,columns=["title","votes","author","url","timestamp","domain","text"])
    df.to_csv("pandaData.csv")


#Makes a BeautifulSoup for the url requested. has a waiting time for 10 sec if an error occurs
# most likely a lot of HTTP 429 errors will occur because of a lot of requests
def makesoup(section_url):
    soup = ""

    while(True):
        try:
            html = urlopen(section_url).read()
            soup = BeautifulSoup(html, "lxml")
            print("made soup for:",section_url)
            break
        except:
            print("sleeping for 10 seconds","waiting for: "+section_url)
            time.sleep(10)

    return soup

#Gets the text for a reddit post
def getPostText(post_url):
    soup = makesoup(post_url)
    for div in soup.find_all("div","expando expando-uninitialized"):
        for text in div.find_all("div","md"):
            return text.get_text()


#Gets post data from main subreddit.
def getPostLinks(section_url,amount,lastpost="",count=0):
    soup = makesoup(section_url)

    #Because all reddit posts and list or autogenerated some regex has to be used
    #Find the list of links from reddit defined form a div with class starting with "thing"
    sitetable = soup.find_all("div",re.compile("^thing"))

    while count<amount:
        for s in sitetable:
            #no reason to get a standard message
            if s["data-author"] !="AutoModerator":
                reddit = []

                #finds the title of the post
                unvoted_entry = s.find_all("div","entry unvoted")
                for child in unvoted_entry:
                    for title in child.find_all("a", re.compile("^title may-blank")):
                        reddit.append(title.string)

                # Finding the amount of votes for the post
                for midcol in s.find_all("div","midcol unvoted"):
                    for vote in midcol.find_all("div","score unvoted"):
                        if vote.string != "â€¢":
                            reddit.append(vote.string)
                        else:
                            reddit.append("unvoted")
                reddit.append(s["data-author"])

                #Creates a proper url for reddit posts
                if str(s["data-url"]).startswith("/r"):
                    print("found reddit post")
                    reddit.append("https://reddit.com"+s["data-url"])
                    local = True
                else:
                    local = False
                    reddit.append(s["data-url"])

                reddit.append(s["data-timestamp"])
                reddit.append(s["data-domain"])

                #if its a reddit post, get the text from it
                if local:
                    reddit.append(getPostText("https://reddit.com"+s["data-url"]))
                else:
                    reddit.append("Outside link")
                lastpost = s["data-fullname"]
                posts.append(reddit)
                count +=1
                if count>=amount:
                    break
            else: continue

        #Recusively call function till for next page
        newurl = BASE_URL+"?count="+str(count)+"&after="+str(lastpost)
        getPostLinks(newurl,amount,lastpost,count)


def scrapeReddit():
    getPostLinks(BASE_URL, NumberOfThreads)
    pandaDataFrama()


#call this function with amount of posts you want.
scrapeReddit()
